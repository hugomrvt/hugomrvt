name: Update recent posts
on:
  schedule:
    # Runs at 06:00 and 18:00 UTC (08:00 and 20:00 in Paris during DST)
    - cron: '0 6,18 * * *'
  workflow_dispatch:
permissions:
  contents: write
jobs:
  update-posts:
    runs-on: ubuntu-latest
    env:
      SUBSTACK_PUBLICATION: genairadar                # <- change if needed
      FETCH_PROXY_URL: ${{ secrets.FETCH_PROXY_URL }} # <- optional Cloudflare/Vercel proxy URL
      # change these two if your README uses different markers
      START_MARKER: '<!-- LATEST_ARTICLES_START -->'
      END_MARKER:   '<!-- LATEST_ARTICLES_END -->'
      # OPTIONAL: set to "1" once to force a refresh even if no new links are detected
      FORCE_README_REFRESH: "1"
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install feedparser requests python-dateutil beautifulsoup4
      - name: Update README with recent posts
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python << 'EOF'
          import feedparser
          import requests
          import re
          import os
          import sys
          import json
          import time
          import calendar
          from datetime import datetime, timezone
          from dateutil import parser as date_parser
          from bs4 import BeautifulSoup
          from urllib.parse import urljoin, urlencode

          # ----------------------------
          # config
          # ----------------------------
          SUBSTACK_PUBLICATION = os.getenv("SUBSTACK_PUBLICATION", "genairadar").strip()
          SUBSTACK_BASE = f"https://{SUBSTACK_PUBLICATION}.substack.com"
          SUBSTACK_FEED = f"{SUBSTACK_BASE}/feed"
          STORIES_BASE = "stories.mrvt.io"
          LOOP_BASE = "loop.genairadar.co"
          INSIGHTS_BASE = "insights.genairadar.co"
          FETCH_PROXY_URL = (os.getenv("FETCH_PROXY_URL") or "").strip()

          # markers (configurable via env)
          START_MARKER = os.getenv("START_MARKER", "<!-- LATEST_ARTICLES_START -->")
          END_MARKER   = os.getenv("END_MARKER",   "<!-- LATEST_ARTICLES_END -->")

          # cache path moved into .utils
          CACHE_DIR = ".utils/cache"
          os.makedirs(CACHE_DIR, exist_ok=True)
          CACHE_JSON = os.path.join(CACHE_DIR, "recent.json")
          CACHE_README = os.path.join(CACHE_DIR, "readme.md")

          # headers
          BROWSER_HEADERS = {
              "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"
                             " AppleWebKit/537.36 (KHTML, like Gecko)"
                             " Chrome/124.0.0.0 Safari/537.36",
              "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
          }
          JSON_HEADERS = {
              **BROWSER_HEADERS,
              "Accept": "application/json, text/plain, */*",
              "Referer": SUBSTACK_BASE + "/archive",
          }

          # ----------------------------
          # utils
          # ----------------------------
          def ensure_aware(dt):
              if dt.tzinfo is None:
                  return dt.replace(tzinfo=timezone.utc)
              return dt

          def to_utc_from_struct(st):
              # struct_time -> aware datetime UTC
              if not st: return None
              try:
                  ts = calendar.timegm(st)
              except Exception:
                  ts = time.mktime(st)
              return datetime.fromtimestamp(ts, tz=timezone.utc)

          def parse_date_flexible(entry):
              for field in ('published_parsed', 'updated_parsed'):
                  st = getattr(entry, field, None)
                  if st:
                      try:
                          return to_utc_from_struct(st)
                      except Exception:
                          pass
              for field in ('published', 'updated', 'created', 'pubDate'):
                  ds = getattr(entry, field, None)
                  if isinstance(ds, str) and ds.strip():
                      try:
                          return ensure_aware(date_parser.parse(ds))
                      except Exception:
                          pass
              return datetime.now(timezone.utc)

          # ----------------------------
          # platform icon
          # ----------------------------
          def detect_platform_logo(url):
              u = url.lower()
              if 'medium.com' in u or 'designsystemscollective.com' in u or 'loop.genairadar.co' in u or 'loop.genairadar.co' in u:
                  return 'Medium', '<img alt="Medium" src="https://www.google.com/s2/favicons?domain=medium.com&sz=20" height="20" width="20"/>'
              elif 'substack.com' in u:
                  return 'Substack', '<img alt="Substack" src="https://www.google.com/s2/favicons?domain=substack.com&sz=20" height="20" width="20"/>'
              else:
                  return 'Blog', 'ðŸ“'

          # ----------------------------
          # proxy-aware HTTP
          # ----------------------------
          def via_proxy(url, accept=None, referer=None, timeout=30):
              if not FETCH_PROXY_URL:
                  return None
              try:
                  params = {"url": url}
                  if accept: params["accept"] = accept
                  if referer: params["referer"] = referer
                  proxy_url = FETCH_PROXY_URL
                  if "?" not in proxy_url:
                      proxy_url = proxy_url.rstrip("/") + "?" + urlencode(params)
                      params = None
                  r = requests.get(proxy_url, headers={"User-Agent": BROWSER_HEADERS["User-Agent"]},
                                   params=params, timeout=timeout)
                  if r.status_code == 200:
                      print(f"Fetched via proxy: {url}")
                      return r
                  print(f"Proxy returned {r.status_code} for {url}")
                  return None
              except Exception as e:
                  print(f"Proxy error for {url}: {e}")
                  return None

          def get_with_retry(url, headers, params=None, max_retries=3, timeout=20, accept=None, referer=None):
              for attempt in range(max_retries + 1):
                  try:
                      r = requests.get(url, headers=headers, params=params, timeout=timeout)
                  except Exception as e:
                      print(f"Request error {url}: {e}")
                      r = None
                  if r and r.status_code == 200:
                      return r
                  if r and r.status_code in (403, 429):
                      if r.status_code == 403:
                          print(f"Access forbidden for {url}")
                          pr = via_proxy(url, accept=accept or headers.get("Accept", "*/*"),
                                         referer=referer or "https://www.google.com/")
                          if pr:
                              return pr
                      else:  # 429
                          wait = 2 ** attempt
                          print(f"Received 429 from {url}, retrying after {wait}s...")
                          time.sleep(wait)
                  else:
                      if r is not None and r.status_code not in (403, 429):
                          if attempt >= max_retries:
                              raise RuntimeError(f"HTTP {r.status_code} for {url}")
                      else:
                          if attempt >= max_retries:
                              return None
              return via_proxy(url, accept=accept or headers.get("Accept", "*/*"),
                               referer=referer or "https://www.google.com/")

          # ----------------------------
          # plan A: Substack JSON archive
          # ----------------------------
          def fetch_substack_archive_api(publication, limit=10):
              api = f"https://{publication}.substack.com/api/v1/archive"
              params = {"sort": "new", "offset": 0, "limit": limit}
              r = get_with_retry(api, JSON_HEADERS, params=params, accept="application/json", referer=SUBSTACK_BASE + "/archive")
              if r is None:
                  print("Archive API not available")
                  return []
              try:
                  data = r.json()
              except Exception as e:
                  print(f"JSON parse error on archive API: {e}")
                  return []
              entries = []
              for item in data:
                  try:
                      t = (item.get("type") or "").lower()
                      if t and t not in ("post", "newsletter", "article", "publication"):
                          continue
                      url = (item.get("canonical_url")
                             or item.get("url")
                             or (f"https://{publication}.substack.com/p/{item.get('slug')}" if item.get("slug") else None))
                      if not url:
                          continue
                      title = item.get("title") or item.get("post_title") or item.get("name") or "No title"
                      date_str = (item.get("post_date") or item.get("published_at")
                                  or item.get("created_at") or item.get("release_date"))
                      try:
                          dt = ensure_aware(date_parser.parse(date_str))
                      except Exception:
                          dt = datetime.now(timezone.utc)
                      entries.append({
                          "title": title,
                          "link": url,
                          "date": dt,
                      })
                  except Exception as e:
                      print(f"Error parsing archive item: {e}")
                      continue
              return entries

          # ----------------------------
          # plan B: Substack RSS
          # ----------------------------
          def fetch_feed_entries(feed_url, limit=10):
              headers = {**BROWSER_HEADERS, "Accept": "application/rss+xml, application/xml;q=0.9, */*;q=0.8"}
              r = get_with_retry(feed_url, headers, accept=headers["Accept"], referer=SUBSTACK_BASE)
              if r is None:
                  print(f"Feed unavailable: {feed_url}")
                  return []
              fp = feedparser.parse(r.content)
              out = []
              for e in fp.entries[:limit]:
                  try:
                      title = getattr(e, 'title', None) or "No title"
                      link = getattr(e, 'link', None) or None
                      if not link:
                          continue
                      dt = parse_date_flexible(e)
                      out.append({
                          "title": title,
                          "link": link,
                          "date": dt,
                      })
                  except Exception as ex:
                      print(f"Error parsing feed entry: {ex}")
                      continue
              return out

          # ----------------------------
          # plan C: parse pages HTML directly
          # ----------------------------
          def fetch_substack_recent_by_html(base_url, limit=10):
              # last resort; try reading /archive, /latest etc.
              urls = [
                  base_url + "/archive",
                  base_url + "/",
              ]
              all_entries = []
              for u in urls:
                  r = get_with_retry(u, BROWSER_HEADERS, accept=BROWSER_HEADERS["Accept"], referer=base_url)
                  if not r: continue
                  soup = BeautifulSoup(r.text, 'html.parser')
                  for a in soup.select('a[href]'):
                      href = a['href']
                      text = a.get_text(strip=True)
                      if not text:
                          continue
                      # very naive filter for substack posts
                      if '/p/' in href or '/post/' in href or '/newsletter/' in href:
                          link = href if href.startswith('http') else urljoin(base_url, href)
                          all_entries.append({
                              'title': text,
                              'link': link,
                              'date': datetime.now(timezone.utc)
                          })
                  if all_entries:
                      break
              return all_entries[:limit]

          def gather_entries():
              entries = []

              # Substack via JSON API first (fast and reliable)
              entries.extend(fetch_substack_archive_api(SUBSTACK_PUBLICATION, limit=10))

              # if needed, additionally pull the RSS feed for redundancy
              if SUBSTACK_FEED:
                  entries.extend(fetch_feed_entries(SUBSTACK_FEED, limit=10))

              # last resort: parse HTML archive pages
              if not entries:
                  entries.extend(fetch_substack_recent_by_html(SUBSTACK_BASE, limit=10))

              # Stories/Medium-like (try feed then rss)
              for feed_url in (f"https://{STORIES_BASE}/feed", f"https://{STORIES_BASE}/rss"):
                  e = fetch_feed_entries(feed_url)
                  if e:
                      entries.extend(e)
                      break

          # Loop.genairadar.co and Insights feeds
          print(f"Fetching Loop feeds...")
          for loop_url in (f"https://{LOOP_BASE}/feed", f"https://{INSIGHTS_BASE}/feed"):
              print(f"Trying feed: {loop_url}")
              e = fetch_feed_entries(loop_url)
              if e:
                  print(f"Found {len(e)} entries from {loop_url}")
                  for entry in e[:3]:  # Log first 3 entries
                      print(f"  - {entry['title'][:50]}... ({entry['date']})")
                  entries.extend(e)
              else:
                  print(f"No entries from {loop_url}")
                            # dedupe by normalized key, prefer newest date
              dedup = {}
              for it in entries:
                  # Normalize key: title lower + link without query params
                  title_key = it.get('title', '').lower().strip()
                  link = it.get('link', '')
                  base_link = link.split('?')[0]  # remove query params
              k = base_link  # dedupe by URL only to allow same titles from different sources 
              d = it.get('date') or datetime.now(timezone.utc) 
              if k not in dedup or d > dedup[k]['date']: 
                              dedup[k] = it
              
              # extract values and sort newest first
              sorted_entries = sorted(dedup.values(), key=lambda x: x['date'], reverse=True)
              return sorted_entries

          def read_readme(path='README.md'):
              with open(path, 'r', encoding='utf-8') as f:
                  return f.read()

          def write_readme(content, path='README.md'):
              with open(path, 'w', encoding='utf-8') as f:
                  f.write(content)

          def format_table_row(title, link, date):
              platform, logo = detect_platform_logo(link)
              date_str = date.strftime('%d/%m/%Y')
              return f"| {logo} {platform} | [{title}]({link}) | {date_str} |"

          def replace_between_markers(content, start_marker, end_marker, new_block):
              start_idx = content.find(start_marker)
              end_idx = content.find(end_marker)
              if start_idx == -1 or end_idx == -1 or end_idx < start_idx:
                  return content
              start_idx += len(start_marker)
              return content[:start_idx] + "\n" + new_block + "\n" + content[end_idx:]

          def commit_needed(old, new):
              # avoid needless commits
              return old.strip() != new.strip()

          def main():
              entries = gather_entries()
              latest = entries[:5]
              # format table
              lines = ['|  | Title | Date |', '|----------|-------|------|']
              for it in latest:
                  lines.append(format_table_row(it['title'], it['link'], it['date']))
              block = "\n".join(lines)

              # read and patch README
              readme = read_readme()
              new_readme = replace_between_markers(readme, START_MARKER, END_MARKER, block)
              # optional forced refresh
              if os.getenv('FORCE_README_REFRESH') == '1':
                  print('Forced refresh of README requested')
                  new_readme = new_readme + "\n"

              if commit_needed(readme, new_readme):
                  write_readme(new_readme)
                  print('README updated with latest posts')
              else:
                  print('No changes to README required')

          if __name__ == '__main__':
              main()
          EOF
