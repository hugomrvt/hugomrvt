name: Update Recent Posts
on:
  schedule:
    # Run twice daily at 6:00 AM and 6:00 PM UTC
    - cron: '0 6,18 * * *'
  workflow_dispatch:
    # Allow manual triggering
jobs:
  update-posts:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install feedparser requests python-dateutil beautifulsoup4
        
    - name: Update README with recent posts
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        python << 'EOF'
        import feedparser
        import requests
        import re
        from datetime import datetime, timezone
        from dateutil import parser as date_parser
        from bs4 import BeautifulSoup
        import os
        from urllib.parse import urljoin, urlparse
        
        def parse_date_flexible(entry):
            """Parse date from RSS entry with multiple fallbacks"""
            # Try different date fields and formats
            date_fields = ['published', 'updated', 'created', 'pubDate']
            parsed_fields = ['published_parsed', 'updated_parsed']
            
            # First try parsed fields
            for field in parsed_fields:
                if hasattr(entry, field) and getattr(entry, field):
                    try:
                        parsed_time = getattr(entry, field)
                        if parsed_time and len(parsed_time) >= 6:
                            return datetime(*parsed_time[:6], tzinfo=timezone.utc)
                    except Exception as e:
                        print(f"Error parsing {field}: {e}")
                        continue
            
            # Then try string date fields with dateutil parser
            for field in date_fields:
                if hasattr(entry, field) and getattr(entry, field):
                    try:
                        date_str = getattr(entry, field)
                        if isinstance(date_str, str):
                            return date_parser.parse(date_str)
                    except Exception as e:
                        print(f"Error parsing {field} string: {e}")
                        continue
            
            # Final fallback - use current time for entries without dates
            print(f"No valid date found for entry: {entry.get('title', 'Unknown')}, using current time")
            return datetime.now(timezone.utc)
        
        def detect_platform_logo(url):
            """Detect platform from URL and return logo HTML"""
            if 'medium.com' in url.lower():
                return 'Medium', '<img src="https://seeklogo.com/images/M/medium-logo-F6BAFE7A6B-seeklogo.com.png" width="20" height="20" alt="Medium">'
            elif 'substack.com' in url.lower():
                return 'Substack', '<img src="https://seeklogo.com/images/S/substack-logo-AE5EF3A7D5-seeklogo.com.png" width="20" height="20" alt="Substack">'
            else:
                return 'Blog', 'üìù'
        
        def scrape_substack_archive():
            """Scrape Substack archive page to extract at least 5 posts"""
            archive_url = 'https://genairadar.substack.com/archive'
            print(f"Scraping Substack archive: {archive_url}")
            
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (compatible; Blog RSS Fetcher)'
                }
                response = requests.get(archive_url, headers=headers, timeout=15)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                entries = []
                
                # Look for Substack post patterns in archive page
                post_selectors = [
                    'a[href*="/p/"]',  # Standard Substack post links
                    '.post-preview-title a',
                    '.frontend-pencraft-Box-module__box a[href*="/p/"]',
                    'h3 a[href*="/p/"]',
                    '.post-title a[href*="/p/"]'
                ]
                
                for selector in post_selectors:
                    post_links = soup.select(selector)
                    if post_links:
                        print(f"Found {len(post_links)} posts with selector: {selector}")
                        
                        for link in post_links:
                            try:
                                title = link.get_text(strip=True)
                                href = link.get('href', '')
                                
                                # Make absolute URL
                                if href and not href.startswith('http'):
                                    href = urljoin(archive_url, href)
                                
                                if title and href and '/p/' in href:
                                    # Try to find publication date
                                    pub_date = datetime.now(timezone.utc)
                                    
                                    # Look for date elements near the link
                                    parent = link.find_parent(['article', 'div', 'section', 'li'])
                                    if parent:
                                        # Look for time elements or date strings
                                        date_elements = parent.find_all(['time', 'span', 'div'], string=re.compile(r'\d{4}|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|\d{1,2}/\d{1,2}/\d{4}|\d{1,2}-\d{1,2}-\d{4}'))
                                        
                                        for date_elem in date_elements:
                                            try:
                                                date_text = date_elem.get_text(strip=True)
                                                if date_elem.get('datetime'):
                                                    date_text = date_elem.get('datetime')
                                                
                                                # Try parsing the date
                                                parsed_date = date_parser.parse(date_text)
                                                # Only use dates that seem reasonable (not too far in future)
                                                if parsed_date <= datetime.now(timezone.utc):
                                                    pub_date = parsed_date
                                                    break
                                            except Exception:
                                                continue
                                    
                                    entries.append({
                                        'title': title,
                                        'link': href,
                                        'date': pub_date,
                                        'source': archive_url
                                    })
                                    print(f"Scraped: {title} - {pub_date.strftime('%d/%m/%Y')}")
                                    
                            except Exception as e:
                                print(f"Error processing scraped post: {e}")
                                continue
                        
                        if len(entries) >= 5:
                            break  # Stop when we have at least 5 entries
                
                print(f"Successfully scraped {len(entries)} entries from archive")
                return entries[:10]  # Limit to 10 entries
                
            except Exception as e:
                print(f"Error scraping Substack archive: {e}")
                return []
        
        def fetch_feed_entries(url, use_fallback=True):
            """Fetch entries from RSS feed with HTML fallback for Substack"""
            try:
                print(f"Fetching feed: {url}")
                feed = feedparser.parse(url)
                
                # Check for RSS parsing issues or empty feed
                has_issues = feed.bozo or not hasattr(feed, 'entries') or len(feed.entries) == 0
                
                if has_issues:
                    print(f"RSS issues detected for {url}: bozo={feed.bozo}, entries={len(getattr(feed, 'entries', []))}")
                    if feed.bozo:
                        print(f"Bozo exception: {feed.bozo_exception}")
                    
                    # Try HTML fallback for Substack
                    if use_fallback and 'substack.com' in url:
                        return scrape_substack_archive()
                
                if not hasattr(feed, 'entries') or not feed.entries:
                    print(f"No entries found in feed: {url}")
                    return []
                
                entries = []
                for entry in feed.entries[:10]:  # Limit to first 10 entries
                    try:
                        pub_date = parse_date_flexible(entry)
                        
                        title = entry.get('title', 'No title')
                        link = entry.get('link', '')
                        
                        if link:  # Only add entries with valid links
                            entries.append({
                                'title': title,
                                'link': link,
                                'date': pub_date,
                                'source': url
                            })
                            print(f"Added entry: {title} - {pub_date}")
                    except Exception as e:
                        print(f"Error processing entry: {e}")
                        continue
                        
                print(f"Successfully fetched {len(entries)} entries from {url}")
                return entries
                
            except Exception as e:
                print(f"Error fetching {url}: {e}")
                return []
        
        def try_multiple_feed_urls(base_urls):
            """Try multiple feed URL variations"""
            all_entries = []
            
            for base_url in base_urls:
                if base_url == 'stories.mrvt.io':
                    # Try /feed and /rss for stories.mrvt.io
                    feed_urls = [
                        f'https://{base_url}/feed',
                        f'https://{base_url}/rss'
                    ]
                    
                    for feed_url in feed_urls:
                        entries = fetch_feed_entries(feed_url, use_fallback=False)
                        if entries:
                            all_entries.extend(entries)
                            break  # Stop trying if we got results
                            
                elif 'substack.com' in base_url:
                    # For Substack, try RSS first, then fallback to archive scraping
                    feed_url = f'{base_url}/feed'
                    entries = fetch_feed_entries(feed_url, use_fallback=True)
                    all_entries.extend(entries)
                    
                else:
                    # Default case
                    entries = fetch_feed_entries(base_url, use_fallback=False)
                    all_entries.extend(entries)
            
            return all_entries
        
        # RSS feed URLs
        FEED_SOURCES = [
            'stories.mrvt.io',
            'https://genairadar.substack.com'
        ]
        
        # Fetch all entries
        all_entries = try_multiple_feed_urls(FEED_SOURCES)
        
        print(f"Total entries fetched: {len(all_entries)}")
        
        # Sort by date (newest first) and take top 5
        if all_entries:
            all_entries.sort(key=lambda x: x['date'], reverse=True)
            top_entries = all_entries[:5]
        else:
            top_entries = []
        
        print(f"Top {len(top_entries)} entries selected")
        
        # Generate markdown table content with logos
        if top_entries:
            content_lines = [
                "| Platform | Title | Date |",
                "|----------|-------|------|"
            ]
            
            for entry in top_entries:
                try:
                    platform, logo = detect_platform_logo(entry['link'])
                    date_str = entry['date'].strftime('%d/%m/%Y')
                    
                    # Clean title for table
                    clean_title = re.sub(r'[|]', '-', entry['title'])
                    
                    content_lines.append(f"| {logo} {platform} | [{clean_title}]({entry['link']}) | {date_str} |")
                except Exception as e:
                    print(f"Error formatting entry: {e}")
                    continue
            
            new_content = '\n'.join(content_lines) + '\n'
        else:
            new_content = "| Platform | Title | Date |\n|----------|-------|------|\n| üìù Blog | No recent articles available | - |\n"
        
        print(f"Generated content:\n{new_content}")
        
        # Define explicit markers
        START_MARKER = '<!-- LATEST_ARTICLES_START -->'
        END_MARKER = '<!-- LATEST_ARTICLES_END -->'
        
        # Update README.md
        readme_file = 'README.md'
        try:
            with open(readme_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            print(f"Original README.md length: {len(content)} characters")
            
            # Check if markers exist
            if START_MARKER not in content:
                # Add markers after the Latest Articles header
                articles_header = '## üìù Latest Articles'
                if articles_header in content:
                    content = content.replace(
                        articles_header,
                        f'{articles_header}\n\n{START_MARKER}\n{END_MARKER}'
                    )
                    print("Added markers to README.md")
                else:
                    print(f"Warning: Could not find '{articles_header}' header in README.md")
            
            # Replace content between markers
            pattern = f'{re.escape(START_MARKER)}.*?{re.escape(END_MARKER)}'
            replacement = f'{START_MARKER}\n{new_content}{END_MARKER}'
            
            updated_content = re.sub(pattern, replacement, content, flags=re.DOTALL)
            
            if updated_content != content:
                with open(readme_file, 'w', encoding='utf-8') as f:
                    f.write(updated_content)
                print(f"Successfully updated {readme_file}")
                print(f"Updated README.md length: {len(updated_content)} characters")
            else:
                print(f"No changes needed for {readme_file}")
                
        except FileNotFoundError:
            print(f"File {readme_file} not found")
        except Exception as e:
            print(f"Error updating {readme_file}: {e}")
        
        EOF
        
    - name: Commit changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add README.md
        
        # Only commit if there are changes
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "feat(workflow): Substack archive fallback + logo table rendering (top-5)"
          git push
        fi
