name: Update recent posts

on:
  schedule:
    # Runs at 06:00 and 18:00 UTC (08:00 and 20:00 in Paris during DST)
    - cron: '0 6,18 * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  update-posts:
    runs-on: ubuntu-latest
    env:
      SUBSTACK_PUBLICATION: genairadar              # <- change if needed
      FETCH_PROXY_URL: ${{ secrets.FETCH_PROXY_URL }} # <- optional Cloudflare/Vercel proxy URL
      # change these two if your README uses different markers
      START_MARKER: '<!-- LATEST_ARTICLES_START -->'
      END_MARKER: '<!-- LATEST_ARTICLES_END -->'
      # OPTIONAL: set to "1" once to force a refresh even if no new links are detected
      # FORCE_README_REFRESH: "1"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install feedparser requests python-dateutil beautifulsoup4

      - name: Update README with recent posts
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python << 'EOF'
          import feedparser
          import requests
          import re
          import os
          import sys
          import json
          import time
          import calendar
          from datetime import datetime, timezone
          from dateutil import parser as date_parser
          from bs4 import BeautifulSoup
          from urllib.parse import urljoin, urlencode

          # ----------------------------
          # config
          # ----------------------------
          SUBSTACK_PUBLICATION = os.getenv("SUBSTACK_PUBLICATION", "genairadar").strip()
          SUBSTACK_BASE = f"https://{SUBSTACK_PUBLICATION}.substack.com"
          SUBSTACK_FEED = f"{SUBSTACK_BASE}/feed"
          STORIES_BASE = "stories.mrvt.io"
          FETCH_PROXY_URL = (os.getenv("FETCH_PROXY_URL") or "").strip()

          # markers (configurable via env)
          START_MARKER = os.getenv("START_MARKER", "<!-- LATEST_ARTICLES_START -->")
          END_MARKER   = os.getenv("END_MARKER",   "<!-- LATEST_ARTICLES_END -->")

          CACHE_FILE = ".posts_cache.json"

          # Browser-like headers
          BROWSER_HEADERS = {
              "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                             "AppleWebKit/537.36 (KHTML, like Gecko) "
                             "Chrome/120.0.0.0 Safari/537.36"),
              "Accept": "application/rss+xml,text/xml;q=0.9,*/*;q=0.8",
              "Accept-Language": "en-US,en;q=0.9,fr;q=0.8",
              "Connection": "close",
          }

          JSON_HEADERS = {
              **BROWSER_HEADERS,
              "Accept": "application/json",
              "Referer": SUBSTACK_BASE + "/archive",
          }

          # ----------------------------
          # cache utils
          # ----------------------------
          def load_cache():
              try:
                  with open(CACHE_FILE, "r", encoding="utf-8") as f:
                      return json.load(f)
              except Exception:
                  return {"known_links": {}, "seen_order": []}

          def save_cache(cache):
              # cap to 500 links to avoid growing indefinitely
              if len(cache.get("seen_order", [])) > 500:
                  to_drop = len(cache["seen_order"]) - 500
                  drop_set = set(cache["seen_order"][:to_drop])
                  cache["seen_order"] = cache["seen_order"][to_drop:]
                  for lk in list(cache["known_links"].keys()):
                      if lk in drop_set:
                          del cache["known_links"][lk]
              with open(CACHE_FILE, "w", encoding="utf-8") as f:
                  json.dump(cache, f, ensure_ascii=False, indent=2)

          def post_key(entry_or_url):
              url = entry_or_url["link"] if isinstance(entry_or_url, dict) else str(entry_or_url)
              return url.split("?")[0].split("#")[0]

          # ----------------------------
          # date helpers
          # ----------------------------
          def to_utc_from_struct(t):
              return datetime.fromtimestamp(calendar.timegm(t), tz=timezone.utc)

          def ensure_aware(dt):
              if dt.tzinfo is None:
                  return dt.replace(tzinfo=timezone.utc)
              return dt

          def parse_date_flexible(entry):
              for field in ('published_parsed', 'updated_parsed'):
                  st = getattr(entry, field, None)
                  if st:
                      try:
                          return to_utc_from_struct(st)
                      except Exception:
                          pass
              for field in ('published', 'updated', 'created', 'pubDate'):
                  ds = getattr(entry, field, None)
                  if isinstance(ds, str) and ds.strip():
                      try:
                          return ensure_aware(date_parser.parse(ds))
                      except Exception:
                          pass
              return datetime.now(timezone.utc)

          # ----------------------------
          # platform icon
          # ----------------------------
          def detect_platform_logo(url):
              u = url.lower()
              if 'medium.com' in u or 'designsystemscollective.com' in u or 'stories.mrvt.io' in u:
                  return 'Medium', '<img alt="Medium" src="https://www.google.com/s2/favicons?domain=medium.com&sz=20" height="20" width="20"/>'
              elif 'substack.com' in u:
                  return 'Substack', '<img alt="Substack" src="https://www.google.com/s2/favicons?domain=substack.com&sz=20" height="20" width="20"/>'
              else:
                  return 'Blog', '📝'

          # ----------------------------
          # proxy-aware HTTP
          # ----------------------------
          def via_proxy(url, accept=None, referer=None, timeout=30):
              if not FETCH_PROXY_URL:
                  return None
              try:
                  params = {"url": url}
                  if accept: params["accept"] = accept
                  if referer: params["referer"] = referer
                  proxy_url = FETCH_PROXY_URL
                  if "?" not in proxy_url:
                      proxy_url = proxy_url.rstrip("/") + "?" + urlencode(params)
                      params = None
                  r = requests.get(proxy_url, headers={"User-Agent": BROWSER_HEADERS["User-Agent"]},
                                   params=params, timeout=timeout)
                  if r.status_code == 200:
                      print(f"Fetched via proxy: {url}")
                      return r
                  print(f"Proxy returned {r.status_code} for {url}")
                  return None
              except Exception as e:
                  print(f"Proxy error for {url}: {e}")
                  return None

          def get_with_retry(url, headers, params=None, max_retries=3, timeout=20, accept=None, referer=None):
              for attempt in range(max_retries + 1):
                  try:
                      r = requests.get(url, headers=headers, params=params, timeout=timeout)
                  except Exception as e:
                      print(f"Request error {url}: {e}")
                      r = None

                  if r and r.status_code == 200:
                      return r

                  if r and r.status_code in (403, 429):
                      if r.status_code == 403:
                          print(f"Access forbidden for {url}")
                          pr = via_proxy(url, accept=accept or headers.get("Accept", "*/*"),
                                         referer=referer or "https://www.google.com/")
                          if pr:
                              return pr
                      else:  # 429
                          wait = 2 ** attempt
                          print(f"Received 429 from {url}, retrying after {wait}s...")
                          time.sleep(wait)
                  else:
                      if r is not None and r.status_code not in (403, 429):
                          if attempt >= max_retries:
                              raise RuntimeError(f"HTTP {r.status_code} for {url}")
                      else:
                          if attempt >= max_retries:
                              return None

              return via_proxy(url, accept=accept or headers.get("Accept", "*/*"),
                               referer=referer or "https://www.google.com/")

          # ----------------------------
          # plan A: Substack JSON archive
          # ----------------------------
          def fetch_substack_archive_api(publication, limit=10):
              api = f"https://{publication}.substack.com/api/v1/archive"
              params = {"sort": "new", "offset": 0, "limit": limit}
              r = get_with_retry(api, JSON_HEADERS, params=params, accept="application/json", referer=SUBSTACK_BASE + "/archive")
              if r is None:
                  print("Archive API not available")
                  return []
              try:
                  data = r.json()
              except Exception as e:
                  print(f"JSON parse error on archive API: {e}")
                  return []

              entries = []
              for item in data:
                  try:
                      t = (item.get("type") or "").lower()
                      if t and t not in ("post", "newsletter", "article", "publication"):
                          continue
                      url = (item.get("canonical_url")
                             or item.get("url")
                             or (f"https://{publication}.substack.com/p/{item.get('slug')}" if item.get("slug") else None))
                      if not url:
                          continue
                      title = item.get("title") or item.get("post_title") or item.get("name") or "No title"
                      date_str = (item.get("post_date") or item.get("published_at")
                                  or item.get("created_at") or item.get("release_date"))
                      try:
                          dt = ensure_aware(date_parser.parse(date_str)) if isinstance(date_str, str) else datetime.now(timezone.utc)
                      except Exception:
                          dt = datetime.now(timezone.utc)
                      entries.append({"title": title, "link": url, "date": dt, "source": api})
                  except Exception as e:
                      print(f"Error on archive item: {e}")
                      continue
              print(f"Archive API returned {len(entries)} entries")
              return entries

          # ----------------------------
          # plan B: RSS
          # ----------------------------
          def parse_feed_with_requests(url):
              r = get_with_retry(url, BROWSER_HEADERS, accept="application/rss+xml", referer=SUBSTACK_BASE)
              if r is None:
                  return None
              return feedparser.parse(r.content)

          def fetch_feed_entries(url):
              try:
                  print(f"Fetching feed: {url}")
                  feed = parse_feed_with_requests(url)
                  if feed is None:
                      print(f"Feed is None for {url}")
                      return []
                  if getattr(feed, "bozo", 0):
                      print(f"RSS issues detected for {url}: {getattr(feed, 'bozo_exception', '')}")
                  if not getattr(feed, "entries", []):
                      print(f"No entries found in feed: {url}")
                      return []
                  entries = []
                  for entry in feed.entries[:10]:
                      try:
                          pub_date = parse_date_flexible(entry)
                          title = entry.get('title', 'No title')
                          link = entry.get('link', '')
                          if link:
                              entries.append({"title": title, "link": link, "date": pub_date, "source": url})
                      except Exception as e:
                          print(f"Error processing entry: {e}")
                          continue
                  print(f"Fetched {len(entries)} entries from {url}")
                  return entries
              except Exception as e:
                  print(f"Error fetching {url}: {e}")
                  return []

          # ----------------------------
          # plan C: HTML archive
          # ----------------------------
          def scrape_substack_archive(base_url):
              archive_url = urljoin(base_url if base_url.endswith("/") else base_url + "/", "archive")
              print(f"Fallback: scraping Substack archive {archive_url}")
              headers = {
                  **BROWSER_HEADERS,
                  "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                  "Referer": "https://www.google.com/",
                  "Cache-Control": "no-cache",
              }
              r = get_with_retry(archive_url, headers, accept=headers["Accept"], referer=headers["Referer"])
              if r is None:
                  return scrape_substack_archive_via_jina(SUBSTACK_PUBLICATION)
              soup = BeautifulSoup(r.content, "html.parser")
              links = soup.select('a[href^="/p/"]')
              seen = set()
              entries = []
              for a in links:
                  href = a.get("href", "")
                  title = a.get_text(strip=True)
                  if not href or "/p/" not in href:
                      continue
                  abs_url = urljoin(base_url, href)
                  if abs_url in seen:
                      continue
                  seen.add(abs_url)
                  if title:
                      entries.append({"title": title, "link": abs_url, "date": datetime.now(timezone.utc), "source": archive_url})
                  if len(entries) >= 10:
                      break
              print(f"Scraped {len(entries)} entries from archive")
              return entries

          # ----------------------------
          # plan D: r.jina.ai last-ditch
          # ----------------------------
          def slug_to_title(slug):
              t = slug.replace('-', ' ').strip()
              return ' '.join(w.capitalize() for w in t.split())

          def resolve_title(url):
              try:
                  r = via_proxy(url, accept="text/html", referer="https://www.google.com/", timeout=20)
                  if r and r.status_code == 200:
                      soup = BeautifulSoup(r.content, "html.parser")
                      title = (soup.title.string or "").strip() if soup.title else ""
                      if title:
                          title = re.sub(r"\s*\|\s*Substack\s*$", "", title, flags=re.I)
                          return title
              except Exception:
                  pass
              return slug_to_title(url.rsplit("/", 1)[-1])

          def scrape_substack_archive_via_jina(publication):
              url = f"https://r.jina.ai/http://{publication}.substack.com/archive"
              print(f"Last-ditch: fetching via r.jina.ai {url}")
              try:
                  r = requests.get(url, headers={"User-Agent": BROWSER_HEADERS["User-Agent"]}, timeout=30)
              except Exception as e:
                  print(f"jina fetch error: {e}")
                  return []
              if r.status_code != 200:
                  print(f"jina returned {r.status_code}")
                  return []
              text = r.text
              pattern = re.compile(rf"https?://{re.escape(publication)}\.substack\.com/p/[a-zA-Z0-9\-]+")
              links = list(dict.fromkeys(pattern.findall(text)))[:10]
              entries = []
              for link in links:
                  title = resolve_title(link)
                  entries.append({"title": title, "link": link, "date": datetime.now(timezone.utc), "source": "r.jina.ai"})
              print(f"jina extracted {len(entries)} entries")
              return entries

          # ----------------------------
          # sources orchestration
          # ----------------------------
          def gather_entries():
              entries = []

              # Substack: JSON → RSS → HTML → r.jina.ai
              entries.extend(fetch_substack_archive_api(SUBSTACK_PUBLICATION, limit=10))
              if not entries:
                  entries.extend(fetch_feed_entries(SUBSTACK_FEED))
              if not entries:
                  entries.extend(scrape_substack_archive(SUBSTACK_BASE))

              # Stories/Medium-like (try feed then rss)
              for feed_url in (f"https://{STORIES_BASE}/feed", f"https://{STORIES_BASE}/rss"):
                  e = fetch_feed_entries(feed_url)
                  if e:
                      entries.extend(e)
                      break

              # dedupe by link key, keep newest date
              by_key = {}
              for e in entries:
                  k = post_key(e)
                  if k not in by_key or e["date"] > by_key[k]["date"]:
                      by_key[k] = e
              return list(by_key.values())

          # ----------------------------
          # fetch
          # ----------------------------
          all_entries = gather_entries()
          print(f"Total entries fetched: {len(all_entries)}")

          if not all_entries:
              print("No entries from any source; skipping README update.")
              if not os.path.exists(CACHE_FILE):
                  save_cache({"known_links": {}, "seen_order": []})
              sys.exit(0)

          # sort by date (newest first)
          all_entries.sort(key=lambda x: x["date"], reverse=True)

          # detect new links via cache
          cache = load_cache()
          known = cache.get("known_links", {})
          seen_order = cache.get("seen_order", [])

          new_links = []
          for e in all_entries:
              k = post_key(e)
              if k not in known:
                  new_links.append(k)

          # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
          # FORCE REFRESH BLOCK (added)
          force = os.getenv("FORCE_README_REFRESH") == "1"
          if not new_links and not force:
              print("No new posts detected; skipping README update.")
              if not os.path.exists(CACHE_FILE):
                  save_cache({"known_links": {}, "seen_order": []})
              sys.exit(0)
          # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

          # update cache
          for e in all_entries:
              k = post_key(e)
              if k not in known:
                  known[k] = e["date"].astimezone(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
                  seen_order.append(k)
          cache["known_links"] = known
          cache["seen_order"] = seen_order
          save_cache(cache)

          # stabilize dates for archive/jina-derived entries using cached ISO date
          for e in all_entries:
              k = post_key(e)
              src = str(e.get("source", "")).lower()
              if ("/archive" in src or "r.jina.ai" in src) and k in known:
                  try:
                      e["date"] = ensure_aware(date_parser.parse(known[k]))
                  except Exception:
                      pass

          # top 5
          top_entries = all_entries[:5]

          # ----------------------------
          # markdown table
          # ----------------------------
          if top_entries:
              content_lines = [
                  "|  | Title | Date |",
                  "|----------|-------|------|"
              ]
              for entry in top_entries:
                  try:
                      platform, logo = detect_platform_logo(entry['link'])
                      date_str = entry['date'].astimezone(timezone.utc).strftime('%d/%m/%Y')
                      clean_title = re.sub(r'[|]', '-', entry['title'])
                      content_lines.append(f"| {logo} {platform} | [{clean_title}]({entry['link']}) | {date_str} |")
                  except Exception as e:
                      print(f"Error formatting entry: {e}")
                      continue
              new_content = '\n'.join(content_lines) + '\n'
          else:
              new_content = "| | Title | Date |\n|----------|-------|------|\n| 📝 Blog | No recent articles available | - |\n"

          print("Generated content:\n" + new_content)

          # ----------------------------
          # README update (uses configurable markers)
          # ----------------------------
          readme_file = 'README.md'

          try:
              with open(readme_file, 'r', encoding='utf-8') as f:
                  content = f.read()
          except FileNotFoundError:
              print(f"{readme_file} not found, creating a minimal one.")
              content = "## 📝 Latest Articles\n\n" + START_MARKER + "\n" + END_MARKER + "\n"

          # ensure markers exist
          if START_MARKER not in content or END_MARKER not in content:
              header = '## 📝 Latest Articles'
              if header in content and START_MARKER not in content:
                  content = content.replace(header, f'{header}\n\n{START_MARKER}\n{END_MARKER}')
              elif START_MARKER not in content:
                  content += f"\n\n{START_MARKER}\n{END_MARKER}\n"

          pattern = f'{re.escape(START_MARKER)}.*?{re.escape(END_MARKER)}'
          replacement = f'{START_MARKER}\n{new_content}{END_MARKER}'
          updated_content = re.sub(pattern, replacement, content, flags=re.DOTALL)

          if updated_content != content:
              with open(readme_file, 'w', encoding='utf-8') as f:
                  f.write(updated_content)
              print(f"Successfully updated {readme_file}")
          else:
              print("README content is unchanged.")

          # ensure cache file always exists
          if not os.path.exists(CACHE_FILE):
              save_cache({"known_links": {}, "seen_order": []})

          EOF

      - name: Commit changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add README.md
          [ -f .posts_cache.json ] && git add .posts_cache.json

          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "chore: update recent posts"
            git push
          fi
