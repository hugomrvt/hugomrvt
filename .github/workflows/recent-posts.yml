name: Update Recent Posts
on:
  schedule:
    # Run twice daily at 6:00 AM and 6:00 PM UTC
    - cron: '0 6,18 * * *'
  workflow_dispatch:
    # Allow manual triggering
jobs:
  update-posts:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install feedparser requests python-dateutil beautifulsoup4
        
    - name: Update README with recent posts
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        python << 'EOF'
        import feedparser
        import requests
        import re
        from datetime import datetime, timezone
        from dateutil import parser as date_parser
        from bs4 import BeautifulSoup
        import os
        from urllib.parse import urljoin, urlparse
        
        def parse_date_flexible(entry):
            """Parse date from RSS entry with multiple fallbacks"""
            # Try different date fields and formats
            date_fields = ['published', 'updated', 'created', 'pubDate']
            parsed_fields = ['published_parsed', 'updated_parsed']
            
            # First try parsed fields
            for field in parsed_fields:
                if hasattr(entry, field) and getattr(entry, field):
                    try:
                        parsed_time = getattr(entry, field)
                        if parsed_time and len(parsed_time) >= 6:
                            return datetime(*parsed_time[:6], tzinfo=timezone.utc)
                    except Exception as e:
                        print(f"Error parsing {field}: {e}")
                        continue
            
            # Then try string date fields with dateutil parser
            for field in date_fields:
                if hasattr(entry, field) and getattr(entry, field):
                    try:
                        date_str = getattr(entry, field)
                        if isinstance(date_str, str):
                            return date_parser.parse(date_str)
                    except Exception as e:
                        print(f"Error parsing {field} string: {e}")
                        continue
            
            # Final fallback - use current time for entries without dates
            print(f"No valid date found for entry: {entry.get('title', 'Unknown')}, using current time")
            return datetime.now(timezone.utc)
        
        def detect_platform_logo(url):
            """Detect platform from URL and return logo HTML with Google s2 favicons"""
            if 'medium.com' in url.lower() or 'designsystemscollective.com' in url.lower() or 'stories.mrvt.io' in url.lower():
                return 'Medium', '<img alt="Medium" src="https://www.google.com/s2/favicons?domain=medium.com&sz=20" height="20" width="20"/>'
            elif 'substack.com' in url.lower():
                return 'Substack', '<img alt="Substack" src="https://www.google.com/s2/favicons?domain=substack.com&sz=20" height="20" width="20"/>'
            else:
                return 'Blog', 'üìù'
        
        def fetch_feed_entries(url, use_fallback=False):
            """Fetch entries from RSS feed with simplified logic for proxy"""
            try:
                print(f"Fetching feed: {url}")
                feed = feedparser.parse(url)
                
                # Check for RSS parsing issues
                if feed.bozo:
                    print(f"RSS issues detected for {url}: {feed.bozo_exception}")
                    if use_fallback:
                        print("Using HTML fallback for Substack")
                        return scrape_substack_archive()
                
                if not hasattr(feed, 'entries') or not feed.entries:
                    print(f"No entries found in feed: {url}")
                    if use_fallback:
                        return scrape_substack_archive()
                    return []
                
                entries = []
                for entry in feed.entries[:10]:  # Limit to first 10 entries
                    try:
                        pub_date = parse_date_flexible(entry)
                        title = entry.get('title', 'No title')
                        link = entry.get('link', '')
                        
                        if link:  # Only add entries with valid links
                            entries.append({
                                'title': title,
                                'link': link,
                                'date': pub_date,
                                'source': url
                            })
                            print(f"Added entry: {title} - {pub_date}")
                    except Exception as e:
                        print(f"Error processing entry: {e}")
                        continue
                        
                print(f"Successfully fetched {len(entries)} entries from {url}")
                return entries
                
            except Exception as e:
                print(f"Error fetching {url}: {e}")
                if use_fallback:
                    return scrape_substack_archive()
                return []
        
        def scrape_substack_archive():
            """Fallback: Scrape Substack archive page"""
            archive_url = 'https://genairadar.substack.com/archive'
            print(f"Fallback: Scraping Substack archive: {archive_url}")
            
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (compatible; Blog RSS Fetcher)'
                }
                response = requests.get(archive_url, headers=headers, timeout=15)
                
                if response.status_code != 200:
                    print(f"Archive request failed with {response.status_code}")
                    return []
                
                soup = BeautifulSoup(response.content, 'html.parser')
                entries = []
                
                # Look for Substack post links
                post_links = soup.select('a[href*="/p/"]')
                
                for link in post_links[:5]:  # Limit to 5 entries
                    try:
                        title = link.get_text(strip=True)
                        href = link.get('href', '')
                        
                        if href and not href.startswith('http'):
                            href = urljoin(archive_url, href)
                        
                        if title and href and '/p/' in href:
                            entries.append({
                                'title': title,
                                'link': href,
                                'date': datetime.now(timezone.utc),
                                'source': archive_url
                            })
                            print(f"Scraped: {title}")
                    except Exception as e:
                        print(f"Error processing scraped post: {e}")
                        continue
                
                print(f"Successfully scraped {len(entries)} entries from archive")
                return entries
                
            except Exception as e:
                print(f"Error scraping Substack archive: {e}")
                return []
        
        def try_multiple_feed_urls(base_urls):
            """Try multiple feed URL variations"""
            all_entries = []
            
            for base_url in base_urls:
                if base_url == 'stories.mrvt.io':
                    # Try /feed and /rss for stories.mrvt.io
                    feed_urls = [
                        f'https://{base_url}/feed',
                        f'https://{base_url}/rss'
                    ]
                    
                    for feed_url in feed_urls:
                        entries = fetch_feed_entries(feed_url, use_fallback=False)
                        if entries:
                            all_entries.extend(entries)
                            break  # Stop trying if we got results
                            
                elif 'substack-rss-proxy.vercel.app' in base_url:
                    # Use the sanitized RSS proxy directly
                    entries = fetch_feed_entries(base_url, use_fallback=True)
                    all_entries.extend(entries)
                    
                else:
                    # Default case
                    entries = fetch_feed_entries(base_url, use_fallback=False)
                    all_entries.extend(entries)
            
            return all_entries
        
        # RSS feed URLs - Updated to use sanitized proxy
        FEED_SOURCES = [
            'stories.mrvt.io',
            'https://substack-rss-proxy.vercel.app/api/feed'
        ]
        
        # Fetch all entries
        all_entries = try_multiple_feed_urls(FEED_SOURCES)
        
        print(f"Total entries fetched: {len(all_entries)}")
        
        # Sort by date (newest first) and take top 5
        if all_entries:
            all_entries.sort(key=lambda x: x['date'], reverse=True)
            top_entries = all_entries[:5]
        else:
            top_entries = []
        
        print(f"Top {len(top_entries)} entries selected")
        
        # Generate markdown table content with logos
        if top_entries:
            content_lines = [
                "| Platform | Title | Date |",
                "|----------|-------|------|"
            ]
            
            for entry in top_entries:
                try:
                    platform, logo = detect_platform_logo(entry['link'])
                    date_str = entry['date'].strftime('%d/%m/%Y')
                    
                    # Clean title for table
                    clean_title = re.sub(r'[|]', '-', entry['title'])
                    
                    content_lines.append(f"| {logo} {platform} | [{clean_title}]({entry['link']}) | {date_str} |")
                except Exception as e:
                    print(f"Error formatting entry: {e}")
                    continue
            
            new_content = '\n'.join(content_lines) + '\n'
        else:
            new_content = "| Platform | Title | Date |\n|----------|-------|------|\n| üìù Blog | No recent articles available | - |\n"
        
        print(f"Generated content:\n{new_content}")
        
        # Define explicit markers
        START_MARKER = '<!-- LATEST_ARTICLES_START -->'
        END_MARKER = '<!-- LATEST_ARTICLES_END -->'
        
        # Update README.md
        readme_file = 'README.md'
        try:
            with open(readme_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            print(f"Original README.md length: {len(content)} characters")
            
            # Check if markers exist
            if START_MARKER not in content:
                # Add markers after the Latest Articles header
                articles_header = '## üìù Latest Articles'
                if articles_header in content:
                    content = content.replace(
                        articles_header,
                        f'{articles_header}\n\n{START_MARKER}\n{END_MARKER}'
                    )
                    print("Added markers to README.md")
                else:
                    print(f"Warning: Could not find '{articles_header}' header in README.md")
            
            # Replace content between markers
            pattern = f'{re.escape(START_MARKER)}.*?{re.escape(END_MARKER)}'
            replacement = f'{START_MARKER}\n{new_content}{END_MARKER}'
            
            updated_content = re.sub(pattern, replacement, content, flags=re.DOTALL)
            
            if updated_content != content:
                with open(readme_file, 'w', encoding='utf-8') as f:
                    f.write(updated_content)
                print(f"Successfully updated {readme_file}")
                print(f"Updated README.md length: {len(updated_content)} characters")
            else:
                print(f"No changes needed for {readme_file}")
                
        except FileNotFoundError:
            print(f"File {readme_file} not found")
        except Exception as e:
            print(f"Error updating {readme_file}: {e}")
        
        EOF
        
    - name: Commit changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add README.md
        
        # Only commit if there are changes
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "feat: use sanitized Substack RSS proxy"
          git push
        fi
