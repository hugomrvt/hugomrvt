name: Update recent posts

on:
  schedule:
    # Runs at 06:00 and 18:00 UTC (08:00 and 20:00 in Paris during DST)
    - cron: '0 6,18 * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  update-posts:
    runs-on: ubuntu-latest
    env:
      SUBSTACK_PUBLICATION: genairadar   # <- change if needed
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install feedparser requests python-dateutil beautifulsoup4

      - name: Update README with recent posts
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python << 'EOF'
          import feedparser
          import requests
          import re
          import os
          import sys
          import json
          import time
          import calendar
          from datetime import datetime, timezone
          from dateutil import parser as date_parser
          from bs4 import BeautifulSoup
          from urllib.parse import urljoin

          # ----------------------------
          # config
          # ----------------------------
          SUBSTACK_PUBLICATION = os.getenv("SUBSTACK_PUBLICATION", "genairadar").strip()
          SUBSTACK_BASE = f"https://{SUBSTACK_PUBLICATION}.substack.com"
          SUBSTACK_FEED = f"{SUBSTACK_BASE}/feed"
          STORIES_BASE = "stories.mrvt.io"

          FEED_SOURCES = [
              STORIES_BASE,        # will try /feed then /rss
              SUBSTACK_FEED,       # native Substack RSS feed (fallback B)
          ]

          CACHE_FILE = ".posts_cache.json"

          # Browser-like headers to avoid anti-bot heuristics
          BROWSER_HEADERS = {
              "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                             "AppleWebKit/537.36 (KHTML, like Gecko) "
                             "Chrome/120.0.0.0 Safari/537.36"),
              "Accept": "application/rss+xml,text/xml;q=0.9,*/*;q=0.8",
              "Accept-Language": "en-US,en;q=0.9,fr;q=0.8",
              "Connection": "close",
          }

          JSON_HEADERS = {
              **BROWSER_HEADERS,
              "Accept": "application/json",
              "Referer": SUBSTACK_BASE + "/archive",
          }

          # ----------------------------
          # cache utils
          # ----------------------------
          def load_cache():
              try:
                  with open(CACHE_FILE, "r", encoding="utf-8") as f:
                      return json.load(f)
              except Exception:
                  return {"known_links": {}, "seen_order": []}

          def save_cache(cache):
              # cap to 500 links to avoid growing indefinitely
              if len(cache.get("seen_order", [])) > 500:
                  to_drop = len(cache["seen_order"]) - 500
                  drop_set = set(cache["seen_order"][:to_drop])
                  cache["seen_order"] = cache["seen_order"][to_drop:]
                  for lk in list(cache["known_links"].keys()):
                      if lk in drop_set:
                          del cache["known_links"][lk]
              with open(CACHE_FILE, "w", encoding="utf-8") as f:
                  json.dump(cache, f, ensure_ascii=False, indent=2)

          def post_key(entry):
              # use the link without query as a stable key
              return entry["link"].split("?")[0]

          # ----------------------------
          # date parsing
          # ----------------------------
          def to_utc_from_struct(t):
              # feedparser often returns struct_time in UTC ‚Üí convert to aware datetime
              return datetime.fromtimestamp(calendar.timegm(t), tz=timezone.utc)

          def ensure_aware(dt):
              # ensure timezone-aware UTC datetime
              if dt.tzinfo is None:
                  return dt.replace(tzinfo=timezone.utc)
              return dt

          def parse_date_flexible(entry):
              # try parsed fields first
              parsed_fields = ['published_parsed', 'updated_parsed']
              for field in parsed_fields:
                  st = getattr(entry, field, None)
                  if st:
                      try:
                          return to_utc_from_struct(st)
                      except Exception:
                          pass

              # then try string fields
              date_fields = ['published', 'updated', 'created', 'pubDate']
              for field in date_fields:
                  ds = getattr(entry, field, None)
                  if isinstance(ds, str) and ds.strip():
                      try:
                          return ensure_aware(date_parser.parse(ds))
                      except Exception:
                          pass

              # final fallback: current time
              return datetime.now(timezone.utc)

          # ----------------------------
          # platform detection (for icon)
          # ----------------------------
          def detect_platform_logo(url):
              u = url.lower()
              if 'medium.com' in u or 'designsystemscollective.com' in u or 'stories.mrvt.io' in u:
                  return 'Medium', '<img alt="Medium" src="https://www.google.com/s2/favicons?domain=medium.com&sz=20" height="20" width="20"/>'
              elif 'substack.com' in u:
                  return 'Substack', '<img alt="Substack" src="https://www.google.com/s2/favicons?domain=substack.com&sz=20" height="20" width="20"/>'
              else:
                  return 'Blog', 'üìù'

          # ----------------------------
          # http helpers (retry/backoff)
          # ----------------------------
          def get_with_retry(url, headers, params=None, max_retries=3, timeout=20):
              for attempt in range(max_retries):
                  r = requests.get(url, headers=headers, params=params, timeout=timeout)
                  if r.status_code == 429:
                      wait = 2 ** attempt
                      print(f"Received 429 from {url}, retrying after {wait}s...")
                      time.sleep(wait)
                      continue
                  if r.status_code == 403:
                      print(f"Access forbidden for {url}")
                      return None
                  if r.status_code != 200:
                      raise RuntimeError(f"HTTP {r.status_code} for {url}")
                  return r
              return None

          # ----------------------------
          # plan A: Substack JSON archive API (robust source)
          # ----------------------------
          def fetch_substack_archive_api(publication, limit=10):
              api = f"https://{publication}.substack.com/api/v1/archive"
              params = {"sort": "new", "offset": 0, "limit": limit}
              r = get_with_retry(api, JSON_HEADERS, params=params)
              if r is None:
                  return []
              try:
                  data = r.json()
              except Exception as e:
                  print(f"JSON parse error on archive API: {e}")
                  return []

              entries = []
              for item in data:
                  try:
                      # URL resolution: prefer canonical_url, else url, else build from slug
                      url = (item.get("canonical_url")
                             or item.get("url")
                             or (f"https://{publication}.substack.com/p/{item.get('slug')}" if item.get("slug") else None))
                      if not url:
                          continue

                      # title resolution: multiple possible keys across deployments
                      title = item.get("title") or item.get("post_title") or item.get("name") or "No title"

                      # date resolution
                      date_str = (item.get("post_date") or item.get("published_at")
                                  or item.get("created_at") or item.get("release_date"))
                      try:
                          dt = ensure_aware(date_parser.parse(date_str)) if isinstance(date_str, str) else datetime.now(timezone.utc)
                      except Exception:
                          dt = datetime.now(timezone.utc)

                      entries.append({
                          "title": title,
                          "link": url,
                          "date": dt,
                          "source": api
                      })
                  except Exception as e:
                      print(f"Error on archive item: {e}")
                      continue

              print(f"Archive API returned {len(entries)} entries")
              return entries

          # ----------------------------
          # plan B: RSS using a browser-like UA
          # ----------------------------
          def parse_feed_with_requests(url):
              r = get_with_retry(url, BROWSER_HEADERS)
              if r is None:
                  return None
              return feedparser.parse(r.content)

          def fetch_feed_entries(url, use_fallback=False, base_for_fallback=None):
              try:
                  print(f"Fetching feed: {url}")
                  feed = parse_feed_with_requests(url)
                  if feed is None:
                      print(f"Feed is None for {url}")
                      if use_fallback and base_for_fallback:
                          return scrape_substack_archive(base_for_fallback)
                      return []

                  if getattr(feed, "bozo", 0):
                      print(f"RSS issues detected for {url}: {getattr(feed, 'bozo_exception', '')}")
                      if use_fallback and base_for_fallback:
                          return scrape_substack_archive(base_for_fallback)

                  if not getattr(feed, "entries", []):
                      print(f"No entries found in feed: {url}")
                      if use_fallback and base_for_fallback:
                          return scrape_substack_archive(base_for_fallback)
                      return []

                  entries = []
                  for entry in feed.entries[:10]:
                      try:
                          pub_date = parse_date_flexible(entry)
                          title = entry.get('title', 'No title')
                          link = entry.get('link', '')
                          if link:
                              entries.append({
                                  'title': title,
                                  'link': link,
                                  'date': pub_date,
                                  'source': url
                              })
                      except Exception as e:
                          print(f"Error processing entry: {e}")
                          continue
                  print(f"Fetched {len(entries)} entries from {url}")
                  return entries

              except Exception as e:
                  print(f"Error fetching {url}: {e}")
                  if use_fallback and base_for_fallback:
                      return scrape_substack_archive(base_for_fallback)
                  return []

          # ----------------------------
          # plan C: Substack archive HTML fallback
          # ----------------------------
          def scrape_substack_archive(base_url):
              archive_url = urljoin(base_url if base_url.endswith("/") else base_url + "/", "archive")
              print(f"Fallback: scraping Substack archive {archive_url}")
              headers = {
                  **BROWSER_HEADERS,
                  "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                  "Referer": "https://www.google.com/",
                  "Cache-Control": "no-cache",
              }
              r = get_with_retry(archive_url, headers)
              if r is None:
                  return []

              soup = BeautifulSoup(r.content, "html.parser")
              # Substack archive pages use anchors to /p/{slug}
              links = soup.select('a[href^="/p/"]')
              seen = set()
              entries = []
              for a in links:
                  href = a.get("href", "")
                  title = a.get_text(strip=True)
                  if not href or "/p/" not in href:
                      continue
                  abs_url = urljoin(base_url, href)
                  if abs_url in seen:
                      continue
                  seen.add(abs_url)
                  if title:
                      entries.append({
                          "title": title,
                          "link": abs_url,
                          "date": datetime.now(timezone.utc),  # archive doesn't expose reliable dates
                          "source": archive_url
                      })
                  if len(entries) >= 10:
                      break
              print(f"Scraped {len(entries)} entries from archive")
              return entries

          # ----------------------------
          # orchestration
          # ----------------------------
          def try_multiple_feed_urls():
              all_entries = []

              # Substack: try JSON API (most reliable), then RSS, then HTML
              substack_json_entries = fetch_substack_archive_api(SUBSTACK_PUBLICATION, limit=10)
              if substack_json_entries:
                  all_entries.extend(substack_json_entries)
              else:
                  # RSS then HTML as fallbacks
                  rss_entries = fetch_feed_entries(SUBSTACK_FEED, use_fallback=False)
                  if rss_entries:
                      all_entries.extend(rss_entries)
                  else:
                      html_entries = scrape_substack_archive(SUBSTACK_BASE)
                      all_entries.extend(html_entries)

              # Stories/Medium-like source
              for feed_url in (f"https://{STORIES_BASE}/feed", f"https://{STORIES_BASE}/rss"):
                  e = fetch_feed_entries(feed_url, use_fallback=False)
                  if e:
                      all_entries.extend(e)
                      break

              return all_entries

          # ----------------------------
          # fetch entries
          # ----------------------------
          all_entries = try_multiple_feed_urls()
          print(f"Total entries fetched: {len(all_entries)}")

          # if nothing found, exit gracefully
          if not all_entries:
              print("No entries from any source; skipping README update.")
              if not os.path.exists(CACHE_FILE):
                  save_cache({"known_links": {}, "seen_order": []})
              sys.exit(0)

          # sort by date (newest first)
          all_entries.sort(key=lambda x: x["date"], reverse=True)

          # detect new links via cache
          cache = load_cache()
          known = cache.get("known_links", {})
          seen_order = cache.get("seen_order", [])

          new_links = []
          for e in all_entries:
              k = post_key(e)
              if k not in known:
                  new_links.append(k)

          if not new_links:
              print("No new posts detected; skipping README update.")
              if not os.path.exists(CACHE_FILE):
                  save_cache({"known_links": {}, "seen_order": []})
              sys.exit(0)

          # update cache with new links
          for e in all_entries:
              k = post_key(e)
              if k not in known:
                  known[k] = e["date"].astimezone(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
                  seen_order.append(k)

          cache["known_links"] = known
          cache["seen_order"] = seen_order
          save_cache(cache)

          # stabilize dates for /archive-derived entries using cached ISO date
          for e in all_entries:
              k = post_key(e)
              src = str(e.get("source", "")).lower()
              if "/archive" in src and k in known:
                  try:
                      e["date"] = ensure_aware(date_parser.parse(known[k]))
                  except Exception:
                      pass

          # take top 5
          top_entries = all_entries[:5]

          # ----------------------------
          # generate markdown table
          # ----------------------------
          if top_entries:
              content_lines = [
                  "| Platform | Title | Date |",
                  "|----------|-------|------|"
              ]
              for entry in top_entries:
                  try:
                      platform, logo = detect_platform_logo(entry['link'])
                      date_str = entry['date'].astimezone(timezone.utc).strftime('%d/%m/%Y')
                      clean_title = re.sub(r'[|]', '-', entry['title'])
                      content_lines.append(f"| {logo} {platform} | [{clean_title}]({entry['link']}) | {date_str} |")
                  except Exception as e:
                      print(f"Error formatting entry: {e}")
                      continue
              new_content = '\n'.join(content_lines) + '\n'
          else:
              new_content = "| Platform | Title | Date |\n|----------|-------|------|\n| üìù Blog | No recent articles available | - |\n"

          print("Generated content:\n" + new_content)

          # ----------------------------
          # update README
          # ----------------------------
          START_MARKER = '<!-- LATEST_ARTICLES_START -->'
          END_MARKER = '<!-- LATEST_ARTICLES_END -->'
          readme_file = 'README.md'

          try:
              with open(readme_file, 'r', encoding='utf-8') as f:
                  content = f.read()
          except FileNotFoundError:
              # create a minimal README if missing
              print(f"{readme_file} not found, creating a minimal one.")
              content = "## üìù Latest Articles\n\n" + START_MARKER + "\n" + END_MARKER + "\n"

          # ensure markers exist
          if START_MARKER not in content or END_MARKER not in content:
              header = '## üìù Latest Articles'
              if header in content and START_MARKER not in content:
                  content = content.replace(
                      header,
                      f'{header}\n\n{START_MARKER}\n{END_MARKER}'
                  )
              elif START_MARKER not in content:
                  content += f"\n\n{START_MARKER}\n{END_MARKER}\n"

          pattern = f'{re.escape(START_MARKER)}.*?{re.escape(END_MARKER)}'
          replacement = f'{START_MARKER}\n{new_content}{END_MARKER}'
          updated_content = re.sub(pattern, replacement, content, flags=re.DOTALL)

          if updated_content != content:
              with open(readme_file, 'w', encoding='utf-8') as f:
                  f.write(updated_content)
              print(f"Successfully updated {readme_file}")
          else:
              print("README content is unchanged.")

          # ensure cache file always exists
          if not os.path.exists(CACHE_FILE):
              save_cache({"known_links": {}, "seen_order": []})

          EOF

      - name: Commit changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add README.md
          [ -f .posts_cache.json ] && git add .posts_cache.json

          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "chore: update recent posts"
            git push
          fi
